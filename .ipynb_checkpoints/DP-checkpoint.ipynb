{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    env is more like a env_model (not env) to return you probs \n",
    "    on different states and rewards. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # delta to record Value:V(s) diff, determines when to stop\n",
    "        delta = 0\n",
    "        \n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = []\n",
    "            \n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                    \n",
    "                    # Calculate the expected value. \n",
    "                    v.append(action_prob * prob * \n",
    "                             (reward + discount_factor * V[next_state]))\n",
    "                    \n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            \n",
    "            V[s] = sum(v)\n",
    "        # Stop evaluating once our value function change\n",
    "        # is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def one_step_lookahead(env, state, V):\n",
    "    \"\"\"\n",
    "    Helper function to calculate the value \n",
    "    for all action in a given state.\n",
    "\n",
    "    Args:\n",
    "        state: state id\n",
    "        V: The value to use as an estimator, Vector of length env.nS\n",
    "\n",
    "    Returns:\n",
    "        A vector of length env.nA containing \n",
    "        the expected value of each action: q(s, a)\n",
    "    \"\"\"\n",
    "    A = np.zeros(env.nA)\n",
    "    for a in range(env.nA):\n",
    "        for prob, next_state, reward, done in env.P[state][a]:\n",
    "            A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, \n",
    "                       policy_eval_fn=policy_eval, \n",
    "                       discount_factor=1.0):\n",
    "    \n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        V = policy_eval_fn(policy, env, discount_factor)\n",
    "        \n",
    "        # Helper variable\n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        for s in range(env.nS):\n",
    "            # The most possible action \n",
    "            # would take under the currect policy\n",
    "            # which is also the best action we believe last iteration\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # Find the best action in this iteration\n",
    "            # by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = one_step_lookahead(env, s, V)\n",
    "            best_a = np.argmax(action_values)\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            \n",
    "            # to choose best_a as next policy for s\n",
    "            # ! mark the implementation here\n",
    "            policy[s] = np.eye(env.nA)[best_a]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. \n",
    "        # Return it\n",
    "        if policy_stable:\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        # helper variable: Stopping condition\n",
    "        delta = 0\n",
    "        \n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(env, s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            \n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            \n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value\n",
    "            \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using V\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "        # or policy[s] = np.eye(env.nA)[best_a]\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamblers_one_step_lookahead(rewards, s, V):\n",
    "    \"\"\"\n",
    "    Helper function to calculate the value for all action in a given state.\n",
    "\n",
    "    Args:\n",
    "        s: The gamblerâ€™s capital. Integer.\n",
    "        V: The vector that contains values at each state. \n",
    "        rewards: The reward vector.\n",
    "\n",
    "    Returns:\n",
    "        A vector containing the expected value of each action. \n",
    "        Its length equals to the number of actions.\n",
    "    \"\"\"\n",
    "    A = np.zeros(101)\n",
    "    stakes = range(1, min(s, 100-s)+1) # Your minimum bet is 1, maximum bet is min(s, 100-s).\n",
    "    for a in stakes:\n",
    "        # rewards[s+a], rewards[s-a] are immediate rewards.\n",
    "        # V[s+a], V[s-a] are values of the next states.\n",
    "        # This is the core of the Bellman equation: The expected value of your action is \n",
    "        # the sum of immediate rewards and the value of the next state.\n",
    "        A[a] = p_h * (rewards[s+a] + V[s+a]*discount_factor) + (1-p_h) * (rewards[s-a] + V[s-a]*discount_factor)\n",
    "    return A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "congyuml",
   "language": "python",
   "name": "congyuml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
