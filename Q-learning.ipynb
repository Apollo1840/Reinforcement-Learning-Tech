{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c3b79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/gym/core.py:27: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from gyms.frozen_lake import FrozenLakeEnvJP\n",
    "\n",
    "\n",
    "class QLearning:\n",
    "    \n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.99):\n",
    "        self.env = env\n",
    "        # Initialize the state-action matrix (Q-table) with zeros\n",
    "        self.state_action_matrix = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.learning_rate = learning_rate  # Alpha\n",
    "        self.discount_factor = discount_factor  # Gamma\n",
    "        self.exploration_rate = exploration_rate  # Epsilon (for exploration)\n",
    "        self.exploration_decay = exploration_decay  # Epsilon decay\n",
    "\n",
    "    def act(self, state):\n",
    "        # Epsilon-greedy policy: with probability exploration_rate, choose a random action\n",
    "        if random.uniform(0, 1) < self.exploration_rate:\n",
    "            return self.env.action_space.sample()  # Random action\n",
    "        else:\n",
    "            # Choose the action with the highest Q-value  for the current state\n",
    "            return self.predict(state)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return np.argmax(self.state_action_matrix[state, :])\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Update the Q-value using the Q-Learning update rule\n",
    "        next_action = self.predict(state)\n",
    "        td_target = reward + self.discount_factor * self.state_action_matrix[next_state, next_action]\n",
    "        td_delta = td_target - self.state_action_matrix[state, action]\n",
    "        self.state_action_matrix[state, action] += self.learning_rate * td_delta\n",
    "\n",
    "        # Decay exploration rate\n",
    "        if done:\n",
    "            self.exploration_rate *= self.exploration_decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c3233d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FrozenLake environment\n",
    "env = FrozenLakeEnvJP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b0743",
   "metadata": {},
   "source": [
    "# one episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203134c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Over!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initialize the QLearning strategy\n",
    "strategy = QLearning(env)\n",
    "\n",
    "# Reset the environment to the initial state\n",
    "state = env.reset()[0]\n",
    "\n",
    "# Render the initial state (not in the FrozenLakeV1 class but showing how to work with Q-Learning)\n",
    "env.render(step_number=0)\n",
    "\n",
    "# Play the game by following the Q-Learning strategy\n",
    "for step in range(100):\n",
    "    action = strategy.act(state)  # Select action based on the current state\n",
    "    next_state, reward, done, truncated, info = env.step(action)  # Apply the action to the environment\n",
    "    \n",
    "    # Render the updated environment after each action\n",
    "    env.render(step_number=step+1)\n",
    "\n",
    "    # Update the Q-Learning state-action matrix\n",
    "    strategy.step(state, action, reward, next_state, done)\n",
    "    \n",
    "    # Transition to the next state\n",
    "    state = next_state\n",
    "\n",
    "    if done or truncated:\n",
    "        print(\"Game Over!\")\n",
    "        time.sleep(1)\n",
    "        break\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28addc0c",
   "metadata": {},
   "source": [
    "# Multiple Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55d8b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors, patches\n",
    "from PIL import Image\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import os\n",
    "from matplotlib.patches import FancyArrowPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97998b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLakeEnvJP2(FrozenLakeEnvJP):\n",
    "\n",
    "    # Function to render the environment map and the state value matrix with arrows\n",
    "    def render(self, state_action_matrix, **kwargs):\n",
    "        # Get the grid layout (env.desc stores the grid description)\n",
    "        grid_array = np.array(self.desc, dtype='str')\n",
    "\n",
    "        # Get the agent's position (env.s gives the current state as a flat index)\n",
    "        player_position = self.s\n",
    "\n",
    "        # Convert the flat position to 2D coordinates (row, col)\n",
    "        grid_size = grid_array.shape[0]\n",
    "        player_row = player_position // grid_size\n",
    "        player_col = player_position % grid_size\n",
    "\n",
    "        # Create a figure with two subplots: one for the environment, one for the state value matrix\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))  # Two side-by-side subplots\n",
    "\n",
    "        # Plot the environment map in the first subplot (ax1)\n",
    "        for row in range(grid_size):\n",
    "            for col in range(grid_size):\n",
    "                ax = ax1.inset_axes([col / grid_size, (grid_size - 1 - row) / grid_size, 1 / grid_size, 1 / grid_size])\n",
    "\n",
    "                # Render different tiles based on the grid content\n",
    "                if grid_array[row, col] == 'S':  # Start/Entrance\n",
    "                    ax.imshow(np.ones((40, 40, 3)))\n",
    "                elif grid_array[row, col] == 'F':  # Frozen tile (safe to walk)\n",
    "                    ax.imshow(np.ones((40, 40, 3)))  # Render white tile (a blank white plate)\n",
    "                elif grid_array[row, col] == 'H':  # Hole\n",
    "                    ax.imshow(self.hole_img)\n",
    "                elif grid_array[row, col] == 'G':  # Goal\n",
    "                    ax.imshow(self.goal_img)\n",
    "\n",
    "                # Hide axis ticks for each grid cell\n",
    "                ax.set_xticks([])  # Hide x-axis ticks\n",
    "                ax.set_yticks([])  # Hide y-axis ticks\n",
    "\n",
    "        # Overlay the player image on the player's position\n",
    "        player_ax = ax1.inset_axes([player_col / grid_size, (grid_size - 1 - player_row) / grid_size, 1 / grid_size, 1 / grid_size])\n",
    "        player_ax.imshow(self.player_img)\n",
    "        player_ax.set_xticks([])  # Hide x-axis ticks\n",
    "        player_ax.set_yticks([])  # Hide y-axis ticks\n",
    "\n",
    "        # Plot the state value matrix in the second subplot (ax2)\n",
    "        state_values = np.max(state_action_matrix, axis=1).reshape(grid_size, grid_size)  # max(Q(s, a)) for each state\n",
    "\n",
    "        cax = ax2.imshow(state_values, cmap='viridis', interpolation='none')\n",
    "        fig.colorbar(cax, ax=ax2)  # Add a color bar to indicate value scale\n",
    "\n",
    "        # Add labels and formatting for the state value matrix\n",
    "        ax2.set_title(\"State Value Matrix (max(Q(s,a)))\")\n",
    "        ax2.set_xticks([])  # Hide x-axis ticks\n",
    "        ax2.set_yticks([])  # Hide y-axis ticks\n",
    "\n",
    "        # Add arrows pointing to the best action\n",
    "        for state in range(state_action_matrix.shape[0]):\n",
    "            row = state // grid_size\n",
    "            col = state % grid_size\n",
    "            best_action = np.argmax(state_action_matrix[state, :])\n",
    "\n",
    "            # Coordinates for the center of the cell\n",
    "            start_x = col\n",
    "            start_y = row \n",
    "            \n",
    "            # Add an arrow in the direction of the best action\n",
    "            if best_action == 0:  # Left\n",
    "                ax2.add_patch(FancyArrowPatch((start_x+0.2, start_y), (start_x - 0.2, start_y), mutation_scale=15, color='white', lw=2))\n",
    "            elif best_action == 1:  # Down\n",
    "                ax2.add_patch(FancyArrowPatch((start_x, start_y - 0.2), (start_x, start_y + 0.2), mutation_scale=15, color='white', lw=2))\n",
    "            elif best_action == 2:  # Right\n",
    "                ax2.add_patch(FancyArrowPatch((start_x-0.2, start_y), (start_x + 0.2, start_y), mutation_scale=15, color='white', lw=2))\n",
    "            elif best_action == 3:  # Up\n",
    "                ax2.add_patch(FancyArrowPatch((start_x, start_y + 0.2), (start_x, start_y - 0.2), mutation_scale=15, color='white', lw=2))\n",
    "\n",
    "        # Set the title for the overall plot\n",
    "        fig.suptitle(f\"Episode: {kwargs.get('episode_number', None)}, Step: {kwargs.get('step_number', None)}\", fontsize=16)\n",
    "\n",
    "        # Show the plot\n",
    "        display(plt.gcf())\n",
    "        clear_output(wait=True)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5b351477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FrozenLake environment\n",
    "env = FrozenLakeEnvJP2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a6206a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the QLearning strategy\n",
    "strategy = QLearning(env, exploration_rate=0.2, learning_rate=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c37be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18 done in 20 steps with total reward: 77\n"
     ]
    }
   ],
   "source": [
    "# Define number of episodes\n",
    "num_episodes = 30\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "# Track the rewards per episode (optional)\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment to the initial state at the start of each episode\n",
    "    state = env.reset()[0]  # Extract the actual state from the reset\n",
    "    \n",
    "    total_reward = 0  # Initialize the reward tracker for this episode\n",
    "\n",
    "    # Play the game by following the Q-Learning strategy\n",
    "    for step in range(max_steps_per_episode):\n",
    "        action = strategy.act(state)  # Select action based on the current state\n",
    "        next_state, reward, done, truncated, info = env.step(action)  # Apply the action to the environment\n",
    "\n",
    "        if done and reward == 1:\n",
    "            reward = 200\n",
    "        elif done:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = next_state\n",
    "\n",
    "        # Update the Q-Learning state-action matrix\n",
    "        strategy.step(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Render the updated environment after each action (optional for multiple episodes)\n",
    "        env.render(strategy.state_action_matrix, step_number=step+1, episode_number=episode+1)\n",
    "        \n",
    "        # Transition to the next state\n",
    "        state = next_state\n",
    "        total_reward += reward  # Accumulate the reward\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode {episode + 1} done in {step + 1} steps with total reward: {total_reward}\")\n",
    "            time.sleep(1)\n",
    "            break\n",
    "        if truncated:\n",
    "            print(f\"Episode {episode + 1} finished in {step + 1} steps with total reward: {total_reward}\")\n",
    "            time.sleep(1)\n",
    "            break\n",
    "\n",
    "    # Log the total reward for this episode\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "# Close the environment after all episodes\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Analyze performance over episodes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the total reward over time to see the agent's improvement\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward per Episode in FrozenLake')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee08695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa3968b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "congyuml",
   "language": "python",
   "name": "congyuml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
